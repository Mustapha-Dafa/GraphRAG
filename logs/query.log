2025-12-11 01:54:13.0648 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:14.0412 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:14.0721 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:14.0881 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:16.0562 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188825, Requested 12801. Please try again in 487ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188825, Requested 12801. Please try again in 487ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188825, Requested 12801. Please try again in 487ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188825, Requested 12801. Please try again in 487ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:16.0629 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:16.0653 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12092. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12092. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12092. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12092. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:17.0096 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199312, Requested 12191. Please try again in 3.45s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199312, Requested 12191. Please try again in 3.45s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199312, Requested 12191. Please try again in 3.45s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199312, Requested 12191. Please try again in 3.45s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:17.0420 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:17.0435 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:17.0871 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:18.0052 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:18.0110 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:18.0280 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:18.0368 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:18.0597 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:25.0086 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:25.0471 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2542. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:25.0573 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:25.0719 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:27.0362 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:27.0418 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:27.0618 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:27.0758 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:28.0015 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:28.0437 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:28.0841 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:29.0032 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:29.0096 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:29.0741 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:38.0161 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:38.0257 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:38.0257 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:38.0550 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:40.0084 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:40.0292 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:40.0426 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:40.0715 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12531. Please try again in 3.759s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:40.0811 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:42.0073 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:42.0089 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:42.0265 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12920. Please try again in 3.876s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:42.0782 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:55.0029 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12451. Please try again in 3.735s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:55.0176 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:55.0401 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12858. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:55.0528 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:56.0789 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12287. Please try again in 3.686s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:56.0986 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12801. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:57.0175 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:57.0793 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12460. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:58.0634 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12576. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:59.0280 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:54:59.0480 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:55:21.0576 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12191. Please try again in 3.657s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:55:24.0145 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12469. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:55:24.0455 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13012. Please try again in 3.903s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:14.0515 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0862 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0870 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0878 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0932 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0948 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:15.0996 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0052 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0073 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0087 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0095 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0100 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0121 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:16.0275 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:18.0782 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:18.0833 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:19.0331 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:19.0427 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:19.0427 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:19.0602 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:19.0977 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:20.0057 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:20.0217 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:20.0541 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0438 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0598 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12671. Please try again in 3.801s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0598 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0614 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12571. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0630 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12090. Please try again in 3.627s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0634 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0662 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12189. Please try again in 3.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0685 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0715 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0734 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12855. Please try again in 3.856s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0834 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:26.0906 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:27.0195 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12446. Please try again in 3.733s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:27.0248 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:29.0781 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198460, Requested 12285. Please try again in 3.223s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198460, Requested 12285. Please try again in 3.223s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198460, Requested 12285. Please try again in 3.223s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198460, Requested 12285. Please try again in 3.223s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:29.0829 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198317, Requested 12529. Please try again in 3.253s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198317, Requested 12529. Please try again in 3.253s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198317, Requested 12529. Please try again in 3.253s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198317, Requested 12529. Please try again in 3.253s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:30.0073 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197828, Requested 11773. Please try again in 2.88s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197828, Requested 11773. Please try again in 2.88s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197828, Requested 11773. Please try again in 2.88s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197828, Requested 11773. Please try again in 2.88s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:30.0266 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 196823, Requested 12457. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 196823, Requested 12457. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 196823, Requested 12457. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 196823, Requested 12457. Please try again in 2.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:30.0580 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195879, Requested 12918. Please try again in 2.639s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195879, Requested 12918. Please try again in 2.639s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195879, Requested 12918. Please try again in 2.639s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195879, Requested 12918. Please try again in 2.639s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:30.0597 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195787, Requested 12574. Please try again in 2.508s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195787, Requested 12574. Please try again in 2.508s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195787, Requested 12574. Please try again in 2.508s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 195787, Requested 12574. Please try again in 2.508s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:30.0986 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 194531, Requested 13009. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 194531, Requested 13009. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 194531, Requested 13009. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 194531, Requested 13009. Please try again in 2.262s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:31.0268 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193429, Requested 12467. Please try again in 1.768s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193429, Requested 12467. Please try again in 1.768s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193429, Requested 12467. Please try again in 1.768s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193429, Requested 12467. Please try again in 1.768s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:31.0364 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193432, Requested 12890. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193432, Requested 12890. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193432, Requested 12890. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 193432, Requested 12890. Please try again in 1.896s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:36.0941 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12270. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:36.0941 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0005 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12614. Please try again in 3.784s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0074 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12449. Please try again in 3.734s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0165 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0197 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0245 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12798. Please try again in 3.839s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:37.0895 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12772. Please try again in 3.831s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:42.0498 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:42.0546 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11773. Please try again in 3.531s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:42.0562 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12285. Please try again in 3.685s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:43.0000 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12457. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:43.0015 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:43.0468 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:43.0656 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13009. Please try again in 3.902s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:43.0809 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12467. Please try again in 3.74s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:44.0229 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12890. Please try again in 3.866s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0018 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188268, Requested 12270. Please try again in 161ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188268, Requested 12270. Please try again in 161ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188268, Requested 12270. Please try again in 161ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188268, Requested 12270. Please try again in 161ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0245 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198572, Requested 12798. Please try again in 3.411s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198572, Requested 12798. Please try again in 3.411s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198572, Requested 12798. Please try again in 3.411s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198572, Requested 12798. Please try again in 3.411s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0245 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199507, Requested 12772. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199507, Requested 12772. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199507, Requested 12772. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199507, Requested 12772. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0389 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198095, Requested 11773. Please try again in 2.96s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198095, Requested 11773. Please try again in 2.96s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198095, Requested 11773. Please try again in 2.96s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198095, Requested 11773. Please try again in 2.96s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0571 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12800. Please try again in 3.84s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:53.0640 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12896. Please try again in 3.868s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:55.0895 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:56.0829 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:56.0845 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12574. Please try again in 3.772s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:57:57.0026 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12918. Please try again in 3.875s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:58:16.0397 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198307, Requested 12798. Please try again in 3.331s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198307, Requested 12798. Please try again in 3.331s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198307, Requested 12798. Please try again in 3.331s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198307, Requested 12798. Please try again in 3.331s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:58:16.0736 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197602, Requested 12918. Please try again in 3.156s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197602, Requested 12918. Please try again in 3.156s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197602, Requested 12918. Please try again in 3.156s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197602, Requested 12918. Please try again in 3.156s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:58:19.0887 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12529. Please try again in 3.758s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 01:58:30.0712 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=6, delay=64.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2540. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:24.0390 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:26.0715 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:26.0796 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0100 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0218 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0297 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0351 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0377 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0505 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0601 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0633 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:27.0960 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12099. Please try again in 3.629s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12099. Please try again in 3.629s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12099. Please try again in 3.629s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12099. Please try again in 3.629s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:28.0200 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:28.0583 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12279. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12279. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12279. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12279. Please try again in 3.683s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:28.0673 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12294. Please try again in 3.688s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12294. Please try again in 3.688s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12294. Please try again in 3.688s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12294. Please try again in 3.688s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:28.0833 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12538. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12538. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12538. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12538. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:29.0300 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:29.0447 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12927. Please try again in 3.878s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12927. Please try again in 3.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12927. Please try again in 3.878s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12927. Please try again in 3.878s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:29.0479 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:29.0849 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:30.0187 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:30.0944 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2549. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2549. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2549. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 2549. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:33.0760 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12476. Please try again in 3.742s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12476. Please try again in 3.742s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12476. Please try again in 3.742s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12476. Please try again in 3.742s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:34.0212 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11782. Please try again in 3.534s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11782. Please try again in 3.534s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11782. Please try again in 3.534s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 11782. Please try again in 3.534s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:35.0230 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199251, Requested 2549. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199251, Requested 2549. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199251, Requested 2549. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199251, Requested 2549. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:35.0300 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199030, Requested 12510. Please try again in 3.462s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199030, Requested 12510. Please try again in 3.462s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199030, Requested 12510. Please try again in 3.462s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199030, Requested 12510. Please try again in 3.462s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:37.0716 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191348, Requested 12905. Please try again in 1.275s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191348, Requested 12905. Please try again in 1.275s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191348, Requested 12905. Please try again in 1.275s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191348, Requested 12905. Please try again in 1.275s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:37.0732 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191530, Requested 12864. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191530, Requested 12864. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191530, Requested 12864. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191530, Requested 12864. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:37.0890 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190785, Requested 12455. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190785, Requested 12455. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190785, Requested 12455. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190785, Requested 12455. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:37.0969 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190578, Requested 12580. Please try again in 947ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190578, Requested 12580. Please try again in 947ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190578, Requested 12580. Please try again in 947ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190578, Requested 12580. Please try again in 947ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0007 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189870, Requested 12680. Please try again in 765ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189870, Requested 12680. Please try again in 765ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189870, Requested 12680. Please try again in 765ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189870, Requested 12680. Please try again in 765ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0067 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189742, Requested 12623. Please try again in 709ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189742, Requested 12623. Please try again in 709ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189742, Requested 12623. Please try again in 709ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189742, Requested 12623. Please try again in 709ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0120 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190053, Requested 12099. Please try again in 645ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190053, Requested 12099. Please try again in 645ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190053, Requested 12099. Please try again in 645ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190053, Requested 12099. Please try again in 645ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0262 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189153, Requested 12781. Please try again in 580ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189153, Requested 12781. Please try again in 580ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189153, Requested 12781. Please try again in 580ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189153, Requested 12781. Please try again in 580ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0357 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189316, Requested 12807. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189316, Requested 12807. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189316, Requested 12807. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 189316, Requested 12807. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0639 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187813, Requested 12458. Please try again in 81ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187813, Requested 12458. Please try again in 81ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187813, Requested 12458. Please try again in 81ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187813, Requested 12458. Please try again in 81ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0655 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188331, Requested 12809. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188331, Requested 12809. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188331, Requested 12809. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 188331, Requested 12809. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:38.0783 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187938, Requested 12198. Please try again in 40ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187938, Requested 12198. Please try again in 40ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187938, Requested 12198. Please try again in 40ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187938, Requested 12198. Please try again in 40ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:39.0108 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199093, Requested 12466. Please try again in 3.467s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199093, Requested 12466. Please try again in 3.467s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199093, Requested 12466. Please try again in 3.467s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199093, Requested 12466. Please try again in 3.467s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:39.0124 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198559, Requested 12899. Please try again in 3.437s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198559, Requested 12899. Please try again in 3.437s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198559, Requested 12899. Please try again in 3.437s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198559, Requested 12899. Please try again in 3.437s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:39.0172 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198930, Requested 12927. Please try again in 3.557s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198930, Requested 12927. Please try again in 3.557s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198930, Requested 12927. Please try again in 3.557s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198930, Requested 12927. Please try again in 3.557s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:39.0433 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197543, Requested 12583. Please try again in 3.037s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197543, Requested 12583. Please try again in 3.037s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197543, Requested 12583. Please try again in 3.037s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197543, Requested 12583. Please try again in 3.037s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:39.0546 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197207, Requested 13018. Please try again in 3.067s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197207, Requested 13018. Please try again in 3.067s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197207, Requested 13018. Please try again in 3.067s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 197207, Requested 13018. Please try again in 3.067s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:47.0783 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12510. Please try again in 3.752s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:48.0689 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12807. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:48.0771 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12899. Please try again in 3.869s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0539 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0934 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191963, Requested 12099. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191963, Requested 12099. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191963, Requested 12099. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 191963, Requested 12099. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0934 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0949 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0949 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:50.0965 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:51.0046 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12905. Please try again in 3.871s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:51.0098 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:51.0113 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:51.0306 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190521, Requested 12809. Please try again in 999ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190521, Requested 12809. Please try again in 999ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190521, Requested 12809. Please try again in 999ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 190521, Requested 12809. Please try again in 999ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:51.0827 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:52.0037 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:52.0244 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199613, Requested 12583. Please try again in 3.658s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199613, Requested 12583. Please try again in 3.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199613, Requested 12583. Please try again in 3.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 199613, Requested 12583. Please try again in 3.658s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:52.0437 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198519, Requested 12927. Please try again in 3.433s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198519, Requested 12927. Please try again in 3.433s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198519, Requested 12927. Please try again in 3.433s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198519, Requested 12927. Please try again in 3.433s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:01:52.0533 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198614, Requested 13018. Please try again in 3.489s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198614, Requested 13018. Please try again in 3.489s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198614, Requested 13018. Please try again in 3.489s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 198614, Requested 13018. Please try again in 3.489s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0238 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0286 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12623. Please try again in 3.786s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0604 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12680. Please try again in 3.804s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0706 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12580. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0738 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12455. Please try again in 3.736s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0786 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0818 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12809. Please try again in 3.842s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:07.0962 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12198. Please try again in 3.659s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:08.0506 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187976, Requested 12466. Please try again in 132ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187976, Requested 12466. Please try again in 132ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187976, Requested 12466. Please try again in 132ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 187976, Requested 12466. Please try again in 132ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:09.0258 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:09.0449 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:31.0378 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12781. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:33.0431 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12466. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:34.0253 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 12583. Please try again in 3.774s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-11 02:02:34.0269 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=5, delay=32.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 835, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 459, in make_openai_chat_completion_request
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 436, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 882, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1643, in wrapper_async
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\utils.py", line 1489, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2329, in exception_type
    raise e
  File "d:\Data Science\test_graph\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 356, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-s78Wk2BH28NnlaT3UFjnr2zg on tokens per min (TPM): Limit 200000, Used 200000, Requested 13018. Please try again in 3.905s. Visit https://platform.openai.com/account/rate-limits to learn more.
