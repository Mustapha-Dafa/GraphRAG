2025-12-16 17:32:06.0915 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 189636, Requested 12825. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 189636, Requested 12825. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 189636, Requested 12825. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 189636, Requested 12825. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 18:41:35.0826 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-16 23:42:24.0567 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-16 23:43:31.0284 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-16 23:46:11.0896 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-16 23:46:55.0859 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-16 23:48:02.0431 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:48:13.0875 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13025. Please try again in 3.907s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:51:32.0563 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:51:43.0997 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12572. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:50.0869 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:50.0979 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:51.0077 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:51.0334 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:51.0879 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:52.0128 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:52:55.0759 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:02.0016 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12458. Please try again in 3.737s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:02.0241 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12675. Please try again in 3.802s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:02.0753 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12570. Please try again in 3.771s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:02.0882 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12864. Please try again in 3.859s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:03.0137 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12853. Please try again in 3.855s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:03.0198 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12409. Please try again in 3.722s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:06.0299 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198374, Requested 12054. Please try again in 3.128s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198374, Requested 12054. Please try again in 3.128s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198374, Requested 12054. Please try again in 3.128s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198374, Requested 12054. Please try again in 3.128s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:12.0786 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196293, Requested 12570. Please try again in 2.658s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196293, Requested 12570. Please try again in 2.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196293, Requested 12570. Please try again in 2.658s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196293, Requested 12570. Please try again in 2.658s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:13.0452 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194325, Requested 12458. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194325, Requested 12458. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194325, Requested 12458. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194325, Requested 12458. Please try again in 2.034s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:15.0454 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12054. Please try again in 3.616s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:16.0533 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196660, Requested 12675. Please try again in 2.8s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196660, Requested 12675. Please try again in 2.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196660, Requested 12675. Please try again in 2.8s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 196660, Requested 12675. Please try again in 2.8s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:53:29.0504 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 190309, Requested 12054. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 190309, Requested 12054. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 190309, Requested 12054. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 190309, Requested 12054. Please try again in 708ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:55:05.0944 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194677, Requested 12424. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194677, Requested 12424. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194677, Requested 12424. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194677, Requested 12424. Please try again in 2.13s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:55:08.0071 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12886. Please try again in 3.865s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12886. Please try again in 3.865s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12886. Please try again in 3.865s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12886. Please try again in 3.865s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:56:42.0367 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:56:46.0497 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12791. Please try again in 3.837s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12791. Please try again in 3.837s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12791. Please try again in 3.837s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12791. Please try again in 3.837s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:56:52.0913 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:58:26.0149 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:58:26.0310 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-16 23:58:38.0100 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195729, Requested 12416. Please try again in 2.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195729, Requested 12416. Please try again in 2.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195729, Requested 12416. Please try again in 2.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195729, Requested 12416. Please try again in 2.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:01:53.0763 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 191452, Requested 12839. Please try again in 1.287s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 191452, Requested 12839. Please try again in 1.287s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 191452, Requested 12839. Please try again in 1.287s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 191452, Requested 12839. Please try again in 1.287s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:02:02.0359 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 187348, Requested 12839. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 187348, Requested 12839. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 187348, Requested 12839. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 187348, Requested 12839. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:08:11.0869 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12845. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12845. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12845. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12845. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:08:15.0657 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12539. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12539. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12539. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12539. Please try again in 3.761s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:09:10.0970 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12645. Please try again in 3.793s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12645. Please try again in 3.793s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12645. Please try again in 3.793s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12645. Please try again in 3.793s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:10:26.0771 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9980. Please try again in 2.994s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9980. Please try again in 2.994s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9980. Please try again in 2.994s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9980. Please try again in 2.994s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:12:00.0813 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2835. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2835. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2835. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2835. Please try again in 850ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:12:48.0100 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1743. Please try again in 522ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1743. Please try again in 522ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1743. Please try again in 522ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1743. Please try again in 522ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:13:23.0411 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:13:28.0324 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:13:35.0244 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2308. Please try again in 692ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:05.0256 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12880. Please try again in 3.864s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12880. Please try again in 3.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12880. Please try again in 3.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12880. Please try again in 3.864s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:05.0280 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:05.0651 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:05.0865 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:13.0774 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12842. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:16.0875 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12846. Please try again in 3.853s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:17.0824 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12863. Please try again in 3.858s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:15:27.0946 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195675, Requested 12863. Please try again in 2.561s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195675, Requested 12863. Please try again in 2.561s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195675, Requested 12863. Please try again in 2.561s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195675, Requested 12863. Please try again in 2.561s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:16:07.0157 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1574. Please try again in 472ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1574. Please try again in 472ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1574. Please try again in 472ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1574. Please try again in 472ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:16:45.0691 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12418. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12418. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12418. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12418. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:16:58.0839 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:17:03.0367 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1580. Please try again in 474ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:17:34.0997 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1410. Please try again in 423ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1410. Please try again in 423ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1410. Please try again in 423ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1410. Please try again in 423ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:02.0642 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:02.0674 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:02.0815 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:04.0477 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:05.0911 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:13.0630 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:14.0019 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12593. Please try again in 3.777s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:14.0266 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:16.0063 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:17.0068 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:26.0437 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198088, Requested 12593. Please try again in 3.204s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198088, Requested 12593. Please try again in 3.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198088, Requested 12593. Please try again in 3.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198088, Requested 12593. Please try again in 3.204s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:26.0830 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12541. Please try again in 3.762s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:28.0106 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12684. Please try again in 3.805s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:28.0930 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12857. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:18:30.0204 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12859. Please try again in 3.857s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:19:19.0501 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:19:24.0106 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2480. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:20:10.0561 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:20:16.0974 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 4811. Please try again in 1.443s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:02.0210 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12774. Please try again in 3.832s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12774. Please try again in 3.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12774. Please try again in 3.832s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12774. Please try again in 3.832s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:34.0223 - WARNING - graphrag.query.structured_search.global_search.search - Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.
2025-12-17 00:21:53.0593 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0134 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0150 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0156 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0439 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12768. Please try again in 3.83s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12768. Please try again in 3.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12768. Please try again in 3.83s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12768. Please try again in 3.83s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0446 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0569 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 2858. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:58.0585 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:59.0134 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:21:59.0302 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:01.0413 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:01.0429 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:02.0156 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:07.0941 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:07.0957 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12465. Please try again in 3.739s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:08.0538 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:08.0707 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198446, Requested 13156. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198446, Requested 13156. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198446, Requested 13156. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198446, Requested 13156. Please try again in 3.48s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:09.0602 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12631. Please try again in 3.789s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:10.0231 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 193367, Requested 12227. Please try again in 1.678s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 193367, Requested 12227. Please try again in 1.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 193367, Requested 12227. Please try again in 1.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 193367, Requested 12227. Please try again in 1.678s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:11.0156 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:11.0761 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12419. Please try again in 3.725s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:12.0343 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12479. Please try again in 3.743s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:16.0407 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12657. Please try again in 3.797s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:21.0261 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12227. Please try again in 3.668s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:22.0364 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:23.0640 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 13156. Please try again in 3.946s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:25.0876 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12357. Please try again in 3.707s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:25.0885 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199116, Requested 12479. Please try again in 3.478s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199116, Requested 12479. Please try again in 3.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199116, Requested 12479. Please try again in 3.478s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199116, Requested 12479. Please try again in 3.478s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:26.0665 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=3, delay=8.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194467, Requested 12419. Please try again in 2.065s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194467, Requested 12419. Please try again in 2.065s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194467, Requested 12419. Please try again in 2.065s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 194467, Requested 12419. Please try again in 2.065s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:36.0357 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12147. Please try again in 3.644s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:22:36.0648 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=4, delay=16.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198000, Requested 12227. Please try again in 3.068s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198000, Requested 12227. Please try again in 3.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198000, Requested 12227. Please try again in 3.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 198000, Requested 12227. Please try again in 3.068s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:23:49.0931 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 7601. Please try again in 2.28s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 7601. Please try again in 2.28s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 7601. Please try again in 2.28s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 7601. Please try again in 2.28s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:24:27.0992 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1388. Please try again in 416ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1388. Please try again in 416ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1388. Please try again in 416ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1388. Please try again in 416ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:08.0174 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12536. Please try again in 3.76s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12536. Please try again in 3.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12536. Please try again in 3.76s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12536. Please try again in 3.76s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:09.0161 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:10.0099 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:10.0178 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:18.0630 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12414. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:19.0440 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12416. Please try again in 3.724s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:20.0490 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12843. Please try again in 3.852s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:25:32.0509 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1515. Please try again in 454ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1515. Please try again in 454ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1515. Please try again in 454ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 1515. Please try again in 454ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:27:00.0968 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:27:10.0271 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9648. Please try again in 2.894s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:27:59.0783 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195907, Requested 4754. Please try again in 198ms. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195907, Requested 4754. Please try again in 198ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195907, Requested 4754. Please try again in 198ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 195907, Requested 4754. Please try again in 198ms. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:28:52.0004 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9265. Please try again in 2.779s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9265. Please try again in 2.779s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9265. Please try again in 2.779s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 9265. Please try again in 2.779s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:30:50.0643 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199090, Requested 9922. Please try again in 2.703s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199090, Requested 9922. Please try again in 2.703s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199090, Requested 9922. Please try again in 2.703s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 199090, Requested 9922. Please try again in 2.703s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:32:27.0151 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 8855. Please try again in 2.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 8855. Please try again in 2.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 8855. Please try again in 2.656s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 8855. Please try again in 2.656s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:34:11.0930 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6949. Please try again in 2.084s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6949. Please try again in 2.084s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6949. Please try again in 2.084s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6949. Please try again in 2.084s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:34:46.0847 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:34:58.0326 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 836, in acompletion
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 883, in acompletion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 12782. Please try again in 3.834s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:35:56.0164 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=1, delay=2.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.
2025-12-17 00:36:03.0954 - ERROR - graphrag.language_model.providers.litellm.services.retry.exponential_retry - ExponentialRetry: Request failed, retrying, retries=2, delay=4.0, max_retries=10, exception=litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.
Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1004, in async_streaming
    headers, response = await self.make_openai_chat_completion_request(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 190, in async_wrapper
    result = await func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 460, in make_openai_chat_completion_request
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 437, in make_openai_chat_completion_request
    await openai_aclient.chat.completions.with_raw_response.create(
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_legacy_response.py", line 381, in wrapped
    return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2678, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 607, in acompletion
    response = await init_response
               ^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 1054, in async_streaming
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\services\retry\exponential_retry.py", line 71, in aretry
    return await func(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\graphrag\language_model\providers\litellm\chat_model.py", line 109, in _base_acompletion
    return await acompletion(**new_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1646, in wrapper_async
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\utils.py", line 1492, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\main.py", line 626, in acompletion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2340, in exception_type
    raise e
  File "d:\final_graphrag\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 367, in exception_type
    raise RateLimitError(
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-7TxXYgvnDDBHxQkLRFGIzKwn on tokens per min (TPM): Limit 200000, Used 200000, Requested 6968. Please try again in 2.09s. Visit https://platform.openai.com/account/rate-limits to learn more.
